import requests
from bs4 import BeautifulSoup
import pandas as pd

class Scraper:
    def __init__(self):
        self.session = requests.Session()

    def retrieve(self, link):
        try:
            page = self.session.get(link)
            soup = BeautifulSoup(page.content, 'html.parser')

            # Find all divs with the class "meta-table__item__value-text"
            value_text_divs = soup.select(".meta-table__item__value-text")

            # Initialize a dictionary to store the extracted data
            extracted_data = {"PSM": "No data", "BuiltYear": "No data", "TotalUnit": "No data"}
            
            # Define the labels you want to extract and map them to the desired CSV column names
            target_labels = {
                "PSM": "PSM",
                "Built Year": "BuiltYear",
                "Total unit": "TotalUnit"
            }

            # Iterate through the value_text_divs and find the following <span> elements
            for value_div in value_text_divs:
                label_elem = value_div.find_previous("div", class_="meta-table_item__label")
                value_span = value_div.find_next("span")

                if label_elem and value_span:
                    label = label_elem.get_text(strip=True)
                    value = value_span.get_text(strip=True)

                    # If the label matches one of the target labels, save it
                    if label in target_labels:
                        extracted_data[target_labels[label]] = value

            return extracted_data
        except Exception as e:
            print(f"Error processing {link}: {e}")
            return None

# Function to read links from a text file
def read_links_from_file(file_path):
    with open(file_path, 'r') as file:
        links = [line.strip() for line in file.readlines()]
    return links

# Function to scrape multiple links and save to CSV
def scrape_to_csv(links, output_file):
    scraper = Scraper()
    data_list = []

    # Process each link
    for link in links:
        data = scraper.retrieve(link)
        if data:
            data_list.append(data)

    # Create a DataFrame from the extracted data and save it to a CSV
    df = pd.DataFrame(data_list, columns=["PSM", "BuiltYear", "TotalUnit"])
    df.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"Data saved to {output_file}, total valid entries: {len(data_list)}")

# Main function to read links from a file and execute scraping
def main():
    # Replace 'condo_links.txt' with your actual text file containing condo links
    file_path = 'condo_links.txt'
    condo_links = read_links_from_file(file_path)
    
    output_file = "condo_data.csv"
    scrape_to_csv(condo_links, output_file)

if __name__ == "__main__":
    main()
